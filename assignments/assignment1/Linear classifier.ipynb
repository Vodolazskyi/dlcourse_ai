{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:29:45.128393Z",
     "start_time": "2020-02-12T19:29:44.998646Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:35:52.332772Z",
     "start_time": "2020-02-12T19:35:52.299859Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:29:49.851592Z",
     "start_time": "2020-02-12T19:29:46.880034Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:29:49.873202Z",
     "start_time": "2020-02-12T19:29:49.853234Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:30:50.482144Z",
     "start_time": "2020-02-12T19:30:50.460519Z"
    }
   },
   "outputs": [],
   "source": [
    "probs = linear_classifier.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifier.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0, 0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:32:11.669755Z",
     "start_time": "2020-02-12T19:32:11.645362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifier.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifier.cross_entropy_loss(probs, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:35:11.174738Z",
     "start_time": "2020-02-12T19:35:11.139042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifier.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifier.softmax_with_cross_entropy(x, 1), np.array([[1, 0, 0]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:36:24.347552Z",
     "start_time": "2020-02-12T19:36:24.307770Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifier.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifier.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifier.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classfier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:36:37.302060Z",
     "start_time": "2020-02-12T19:36:37.263372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifier.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifier.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:36:45.658334Z",
     "start_time": "2020-02-12T19:36:45.627111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifier.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifier.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:36:54.641821Z",
     "start_time": "2020-02-12T19:36:53.630556Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.397363\n",
      "Epoch 1, loss: 2.330354\n",
      "Epoch 2, loss: 2.311002\n",
      "Epoch 3, loss: 2.303897\n",
      "Epoch 4, loss: 2.303257\n",
      "Epoch 5, loss: 2.302898\n",
      "Epoch 6, loss: 2.302564\n",
      "Epoch 7, loss: 2.301815\n",
      "Epoch 8, loss: 2.301252\n",
      "Epoch 9, loss: 2.301256\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifier.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:36:56.710104Z",
     "start_time": "2020-02-12T19:36:56.563673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f66b17a95b0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:37:12.399138Z",
     "start_time": "2020-02-12T19:37:01.074452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301800\n",
      "Epoch 1, loss: 2.302558\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303309\n",
      "Epoch 4, loss: 2.302126\n",
      "Epoch 5, loss: 2.301524\n",
      "Epoch 6, loss: 2.301150\n",
      "Epoch 7, loss: 2.301359\n",
      "Epoch 8, loss: 2.302305\n",
      "Epoch 9, loss: 2.302205\n",
      "Epoch 10, loss: 2.301595\n",
      "Epoch 11, loss: 2.302039\n",
      "Epoch 12, loss: 2.301247\n",
      "Epoch 13, loss: 2.301836\n",
      "Epoch 14, loss: 2.302396\n",
      "Epoch 15, loss: 2.302299\n",
      "Epoch 16, loss: 2.301161\n",
      "Epoch 17, loss: 2.302058\n",
      "Epoch 18, loss: 2.302180\n",
      "Epoch 19, loss: 2.302339\n",
      "Epoch 20, loss: 2.302694\n",
      "Epoch 21, loss: 2.301872\n",
      "Epoch 22, loss: 2.302261\n",
      "Epoch 23, loss: 2.302209\n",
      "Epoch 24, loss: 2.302215\n",
      "Epoch 25, loss: 2.301591\n",
      "Epoch 26, loss: 2.301535\n",
      "Epoch 27, loss: 2.302557\n",
      "Epoch 28, loss: 2.301689\n",
      "Epoch 29, loss: 2.301815\n",
      "Epoch 30, loss: 2.301483\n",
      "Epoch 31, loss: 2.301553\n",
      "Epoch 32, loss: 2.301640\n",
      "Epoch 33, loss: 2.301991\n",
      "Epoch 34, loss: 2.302104\n",
      "Epoch 35, loss: 2.301741\n",
      "Epoch 36, loss: 2.302652\n",
      "Epoch 37, loss: 2.303017\n",
      "Epoch 38, loss: 2.301521\n",
      "Epoch 39, loss: 2.301504\n",
      "Epoch 40, loss: 2.301565\n",
      "Epoch 41, loss: 2.302361\n",
      "Epoch 42, loss: 2.302223\n",
      "Epoch 43, loss: 2.302385\n",
      "Epoch 44, loss: 2.302532\n",
      "Epoch 45, loss: 2.301950\n",
      "Epoch 46, loss: 2.301686\n",
      "Epoch 47, loss: 2.301881\n",
      "Epoch 48, loss: 2.302291\n",
      "Epoch 49, loss: 2.302069\n",
      "Epoch 50, loss: 2.301971\n",
      "Epoch 51, loss: 2.302533\n",
      "Epoch 52, loss: 2.302043\n",
      "Epoch 53, loss: 2.302003\n",
      "Epoch 54, loss: 2.301729\n",
      "Epoch 55, loss: 2.301272\n",
      "Epoch 56, loss: 2.302220\n",
      "Epoch 57, loss: 2.301765\n",
      "Epoch 58, loss: 2.301959\n",
      "Epoch 59, loss: 2.302229\n",
      "Epoch 60, loss: 2.301372\n",
      "Epoch 61, loss: 2.302540\n",
      "Epoch 62, loss: 2.301929\n",
      "Epoch 63, loss: 2.301502\n",
      "Epoch 64, loss: 2.302201\n",
      "Epoch 65, loss: 2.302674\n",
      "Epoch 66, loss: 2.302191\n",
      "Epoch 67, loss: 2.302989\n",
      "Epoch 68, loss: 2.301388\n",
      "Epoch 69, loss: 2.301621\n",
      "Epoch 70, loss: 2.301533\n",
      "Epoch 71, loss: 2.302804\n",
      "Epoch 72, loss: 2.301514\n",
      "Epoch 73, loss: 2.300962\n",
      "Epoch 74, loss: 2.302366\n",
      "Epoch 75, loss: 2.301895\n",
      "Epoch 76, loss: 2.301768\n",
      "Epoch 77, loss: 2.302649\n",
      "Epoch 78, loss: 2.302394\n",
      "Epoch 79, loss: 2.301415\n",
      "Epoch 80, loss: 2.302148\n",
      "Epoch 81, loss: 2.302263\n",
      "Epoch 82, loss: 2.301172\n",
      "Epoch 83, loss: 2.302803\n",
      "Epoch 84, loss: 2.301892\n",
      "Epoch 85, loss: 2.302788\n",
      "Epoch 86, loss: 2.302016\n",
      "Epoch 87, loss: 2.302450\n",
      "Epoch 88, loss: 2.302002\n",
      "Epoch 89, loss: 2.302363\n",
      "Epoch 90, loss: 2.301257\n",
      "Epoch 91, loss: 2.302553\n",
      "Epoch 92, loss: 2.301951\n",
      "Epoch 93, loss: 2.300910\n",
      "Epoch 94, loss: 2.302318\n",
      "Epoch 95, loss: 2.301563\n",
      "Epoch 96, loss: 2.302494\n",
      "Epoch 97, loss: 2.301579\n",
      "Epoch 98, loss: 2.302468\n",
      "Epoch 99, loss: 2.302352\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:42:58.250801Z",
     "start_time": "2020-02-12T19:39:06.438562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301406\n",
      "Epoch 1, loss: 2.300645\n",
      "Epoch 2, loss: 2.301930\n",
      "Epoch 3, loss: 2.300194\n",
      "Epoch 4, loss: 2.296883\n",
      "Epoch 5, loss: 2.297291\n",
      "Epoch 6, loss: 2.293188\n",
      "Epoch 7, loss: 2.297095\n",
      "Epoch 8, loss: 2.290985\n",
      "Epoch 9, loss: 2.289429\n",
      "Epoch 10, loss: 2.289858\n",
      "Epoch 11, loss: 2.289787\n",
      "Epoch 12, loss: 2.291270\n",
      "Epoch 13, loss: 2.294006\n",
      "Epoch 14, loss: 2.290483\n",
      "Epoch 15, loss: 2.281833\n",
      "Epoch 16, loss: 2.288653\n",
      "Epoch 17, loss: 2.280844\n",
      "Epoch 18, loss: 2.286164\n",
      "Epoch 19, loss: 2.283487\n",
      "Epoch 20, loss: 2.280167\n",
      "Epoch 21, loss: 2.281047\n",
      "Epoch 22, loss: 2.282983\n",
      "Epoch 23, loss: 2.283910\n",
      "Epoch 24, loss: 2.285083\n",
      "Epoch 25, loss: 2.279782\n",
      "Epoch 26, loss: 2.276024\n",
      "Epoch 27, loss: 2.280392\n",
      "Epoch 28, loss: 2.280157\n",
      "Epoch 29, loss: 2.286447\n",
      "Epoch 30, loss: 2.276390\n",
      "Epoch 31, loss: 2.277262\n",
      "Epoch 32, loss: 2.271290\n",
      "Epoch 33, loss: 2.273131\n",
      "Epoch 34, loss: 2.270356\n",
      "Epoch 35, loss: 2.275901\n",
      "Epoch 36, loss: 2.266546\n",
      "Epoch 37, loss: 2.280699\n",
      "Epoch 38, loss: 2.273933\n",
      "Epoch 39, loss: 2.270175\n",
      "Epoch 40, loss: 2.266119\n",
      "Epoch 41, loss: 2.267603\n",
      "Epoch 42, loss: 2.266981\n",
      "Epoch 43, loss: 2.276082\n",
      "Epoch 44, loss: 2.264551\n",
      "Epoch 45, loss: 2.263212\n",
      "Epoch 46, loss: 2.256343\n",
      "Epoch 47, loss: 2.265754\n",
      "Epoch 48, loss: 2.273327\n",
      "Epoch 49, loss: 2.271195\n",
      "Epoch 50, loss: 2.272021\n",
      "Epoch 51, loss: 2.268014\n",
      "Epoch 52, loss: 2.260603\n",
      "Epoch 53, loss: 2.258334\n",
      "Epoch 54, loss: 2.249324\n",
      "Epoch 55, loss: 2.274822\n",
      "Epoch 56, loss: 2.257045\n",
      "Epoch 57, loss: 2.254233\n",
      "Epoch 58, loss: 2.267611\n",
      "Epoch 59, loss: 2.256048\n",
      "Epoch 60, loss: 2.259698\n",
      "Epoch 61, loss: 2.259733\n",
      "Epoch 62, loss: 2.264856\n",
      "Epoch 63, loss: 2.242543\n",
      "Epoch 64, loss: 2.259687\n",
      "Epoch 65, loss: 2.258583\n",
      "Epoch 66, loss: 2.251748\n",
      "Epoch 67, loss: 2.252709\n",
      "Epoch 68, loss: 2.247482\n",
      "Epoch 69, loss: 2.265438\n",
      "Epoch 70, loss: 2.251024\n",
      "Epoch 71, loss: 2.254790\n",
      "Epoch 72, loss: 2.266002\n",
      "Epoch 73, loss: 2.252680\n",
      "Epoch 74, loss: 2.261131\n",
      "Epoch 75, loss: 2.240917\n",
      "Epoch 76, loss: 2.252267\n",
      "Epoch 77, loss: 2.249231\n",
      "Epoch 78, loss: 2.255310\n",
      "Epoch 79, loss: 2.247037\n",
      "Epoch 80, loss: 2.249575\n",
      "Epoch 81, loss: 2.257609\n",
      "Epoch 82, loss: 2.250038\n",
      "Epoch 83, loss: 2.224896\n",
      "Epoch 84, loss: 2.251718\n",
      "Epoch 85, loss: 2.243551\n",
      "Epoch 86, loss: 2.238967\n",
      "Epoch 87, loss: 2.234535\n",
      "Epoch 88, loss: 2.248531\n",
      "Epoch 89, loss: 2.236178\n",
      "Epoch 90, loss: 2.244513\n",
      "Epoch 91, loss: 2.242432\n",
      "Epoch 92, loss: 2.245248\n",
      "Epoch 93, loss: 2.234546\n",
      "Epoch 94, loss: 2.239910\n",
      "Epoch 95, loss: 2.244802\n",
      "Epoch 96, loss: 2.210454\n",
      "Epoch 97, loss: 2.249350\n",
      "Epoch 98, loss: 2.239758\n",
      "Epoch 99, loss: 2.244082\n",
      "Epoch 100, loss: 2.221691\n",
      "Epoch 101, loss: 2.248739\n",
      "Epoch 102, loss: 2.228661\n",
      "Epoch 103, loss: 2.228063\n",
      "Epoch 104, loss: 2.249757\n",
      "Epoch 105, loss: 2.227843\n",
      "Epoch 106, loss: 2.224473\n",
      "Epoch 107, loss: 2.231831\n",
      "Epoch 108, loss: 2.240563\n",
      "Epoch 109, loss: 2.226900\n",
      "Epoch 110, loss: 2.226218\n",
      "Epoch 111, loss: 2.244918\n",
      "Epoch 112, loss: 2.218916\n",
      "Epoch 113, loss: 2.234159\n",
      "Epoch 114, loss: 2.236765\n",
      "Epoch 115, loss: 2.242336\n",
      "Epoch 116, loss: 2.252394\n",
      "Epoch 117, loss: 2.205453\n",
      "Epoch 118, loss: 2.228494\n",
      "Epoch 119, loss: 2.240253\n",
      "Epoch 120, loss: 2.254768\n",
      "Epoch 121, loss: 2.217585\n",
      "Epoch 122, loss: 2.234765\n",
      "Epoch 123, loss: 2.225219\n",
      "Epoch 124, loss: 2.226004\n",
      "Epoch 125, loss: 2.221061\n",
      "Epoch 126, loss: 2.217852\n",
      "Epoch 127, loss: 2.207327\n",
      "Epoch 128, loss: 2.223436\n",
      "Epoch 129, loss: 2.209185\n",
      "Epoch 130, loss: 2.205913\n",
      "Epoch 131, loss: 2.220293\n",
      "Epoch 132, loss: 2.220553\n",
      "Epoch 133, loss: 2.230407\n",
      "Epoch 134, loss: 2.222130\n",
      "Epoch 135, loss: 2.221370\n",
      "Epoch 136, loss: 2.203137\n",
      "Epoch 137, loss: 2.203642\n",
      "Epoch 138, loss: 2.219371\n",
      "Epoch 139, loss: 2.221810\n",
      "Epoch 140, loss: 2.218164\n",
      "Epoch 141, loss: 2.206595\n",
      "Epoch 142, loss: 2.222649\n",
      "Epoch 143, loss: 2.238167\n",
      "Epoch 144, loss: 2.213332\n",
      "Epoch 145, loss: 2.223988\n",
      "Epoch 146, loss: 2.223068\n",
      "Epoch 147, loss: 2.224065\n",
      "Epoch 148, loss: 2.221519\n",
      "Epoch 149, loss: 2.200332\n",
      "Epoch 150, loss: 2.231181\n",
      "Epoch 151, loss: 2.230520\n",
      "Epoch 152, loss: 2.223845\n",
      "Epoch 153, loss: 2.223682\n",
      "Epoch 154, loss: 2.221008\n",
      "Epoch 155, loss: 2.224618\n",
      "Epoch 156, loss: 2.229913\n",
      "Epoch 157, loss: 2.228165\n",
      "Epoch 158, loss: 2.218339\n",
      "Epoch 159, loss: 2.206259\n",
      "Epoch 160, loss: 2.204489\n",
      "Epoch 161, loss: 2.200599\n",
      "Epoch 162, loss: 2.201935\n",
      "Epoch 163, loss: 2.198468\n",
      "Epoch 164, loss: 2.235315\n",
      "Epoch 165, loss: 2.204200\n",
      "Epoch 166, loss: 2.199713\n",
      "Epoch 167, loss: 2.202555\n",
      "Epoch 168, loss: 2.211138\n",
      "Epoch 169, loss: 2.199797\n",
      "Epoch 170, loss: 2.185770\n",
      "Epoch 171, loss: 2.205293\n",
      "Epoch 172, loss: 2.198532\n",
      "Epoch 173, loss: 2.229105\n",
      "Epoch 174, loss: 2.199349\n",
      "Epoch 175, loss: 2.218395\n",
      "Epoch 176, loss: 2.200593\n",
      "Epoch 177, loss: 2.225071\n",
      "Epoch 178, loss: 2.206651\n",
      "Epoch 179, loss: 2.199700\n",
      "Epoch 180, loss: 2.219222\n",
      "Epoch 181, loss: 2.224384\n",
      "Epoch 182, loss: 2.222611\n",
      "Epoch 183, loss: 2.199459\n",
      "Epoch 184, loss: 2.223747\n",
      "Epoch 185, loss: 2.203679\n",
      "Epoch 186, loss: 2.208729\n",
      "Epoch 187, loss: 2.224303\n",
      "Epoch 188, loss: 2.193616\n",
      "Epoch 189, loss: 2.191248\n",
      "Epoch 190, loss: 2.199769\n",
      "Epoch 191, loss: 2.228104\n",
      "Epoch 192, loss: 2.221316\n",
      "Epoch 193, loss: 2.175200\n",
      "Epoch 194, loss: 2.191057\n",
      "Epoch 195, loss: 2.216325\n",
      "Epoch 196, loss: 2.201197\n",
      "Epoch 197, loss: 2.202789\n",
      "Epoch 198, loss: 2.214714\n",
      "Epoch 199, loss: 2.177576\n",
      "Epoch 0, loss: 2.301735\n",
      "Epoch 1, loss: 2.300631\n",
      "Epoch 2, loss: 2.300800\n",
      "Epoch 3, loss: 2.298724\n",
      "Epoch 4, loss: 2.296473\n",
      "Epoch 5, loss: 2.295688\n",
      "Epoch 6, loss: 2.291718\n",
      "Epoch 7, loss: 2.291420\n",
      "Epoch 8, loss: 2.296152\n",
      "Epoch 9, loss: 2.292431\n",
      "Epoch 10, loss: 2.290487\n",
      "Epoch 11, loss: 2.292988\n",
      "Epoch 12, loss: 2.293110\n",
      "Epoch 13, loss: 2.295674\n",
      "Epoch 14, loss: 2.284950\n",
      "Epoch 15, loss: 2.292662\n",
      "Epoch 16, loss: 2.285459\n",
      "Epoch 17, loss: 2.288208\n",
      "Epoch 18, loss: 2.289303\n",
      "Epoch 19, loss: 2.286568\n",
      "Epoch 20, loss: 2.279520\n",
      "Epoch 21, loss: 2.286209\n",
      "Epoch 22, loss: 2.283697\n",
      "Epoch 23, loss: 2.283699\n",
      "Epoch 24, loss: 2.287409\n",
      "Epoch 25, loss: 2.282701\n",
      "Epoch 26, loss: 2.280520\n",
      "Epoch 27, loss: 2.271675\n",
      "Epoch 28, loss: 2.281183\n",
      "Epoch 29, loss: 2.273227\n",
      "Epoch 30, loss: 2.267387\n",
      "Epoch 31, loss: 2.278824\n",
      "Epoch 32, loss: 2.283330\n",
      "Epoch 33, loss: 2.267435\n",
      "Epoch 34, loss: 2.270530\n",
      "Epoch 35, loss: 2.271754\n",
      "Epoch 36, loss: 2.261433\n",
      "Epoch 37, loss: 2.275842\n",
      "Epoch 38, loss: 2.264085\n",
      "Epoch 39, loss: 2.272768\n",
      "Epoch 40, loss: 2.260092\n",
      "Epoch 41, loss: 2.276123\n",
      "Epoch 42, loss: 2.272774\n",
      "Epoch 43, loss: 2.262581\n",
      "Epoch 44, loss: 2.272295\n",
      "Epoch 45, loss: 2.270626\n",
      "Epoch 46, loss: 2.269500\n",
      "Epoch 47, loss: 2.272172\n",
      "Epoch 48, loss: 2.260123\n",
      "Epoch 49, loss: 2.269038\n",
      "Epoch 50, loss: 2.250911\n",
      "Epoch 51, loss: 2.257419\n",
      "Epoch 52, loss: 2.253854\n",
      "Epoch 53, loss: 2.271108\n",
      "Epoch 54, loss: 2.249012\n",
      "Epoch 55, loss: 2.257046\n",
      "Epoch 56, loss: 2.271037\n",
      "Epoch 57, loss: 2.254823\n",
      "Epoch 58, loss: 2.264234\n",
      "Epoch 59, loss: 2.256530\n",
      "Epoch 60, loss: 2.265178\n",
      "Epoch 61, loss: 2.249248\n",
      "Epoch 62, loss: 2.251332\n",
      "Epoch 63, loss: 2.263879\n",
      "Epoch 64, loss: 2.251961\n",
      "Epoch 65, loss: 2.267829\n",
      "Epoch 66, loss: 2.250101\n",
      "Epoch 67, loss: 2.259753\n",
      "Epoch 68, loss: 2.268357\n",
      "Epoch 69, loss: 2.246049\n",
      "Epoch 70, loss: 2.254459\n",
      "Epoch 71, loss: 2.253825\n",
      "Epoch 72, loss: 2.234906\n",
      "Epoch 73, loss: 2.253465\n",
      "Epoch 74, loss: 2.260838\n",
      "Epoch 75, loss: 2.246476\n",
      "Epoch 76, loss: 2.257912\n",
      "Epoch 77, loss: 2.243323\n",
      "Epoch 78, loss: 2.231006\n",
      "Epoch 79, loss: 2.248677\n",
      "Epoch 80, loss: 2.246533\n",
      "Epoch 81, loss: 2.247616\n",
      "Epoch 82, loss: 2.247604\n",
      "Epoch 83, loss: 2.241386\n",
      "Epoch 84, loss: 2.257233\n",
      "Epoch 85, loss: 2.237890\n",
      "Epoch 86, loss: 2.245533\n",
      "Epoch 87, loss: 2.252812\n",
      "Epoch 88, loss: 2.245231\n",
      "Epoch 89, loss: 2.248712\n",
      "Epoch 90, loss: 2.225527\n",
      "Epoch 91, loss: 2.246837\n",
      "Epoch 92, loss: 2.225823\n",
      "Epoch 93, loss: 2.250501\n",
      "Epoch 94, loss: 2.241522\n",
      "Epoch 95, loss: 2.241488\n",
      "Epoch 96, loss: 2.235932\n",
      "Epoch 97, loss: 2.230116\n",
      "Epoch 98, loss: 2.237242\n",
      "Epoch 99, loss: 2.259864\n",
      "Epoch 100, loss: 2.251262\n",
      "Epoch 101, loss: 2.244797\n",
      "Epoch 102, loss: 2.237132\n",
      "Epoch 103, loss: 2.241769\n",
      "Epoch 104, loss: 2.256696\n",
      "Epoch 105, loss: 2.235411\n",
      "Epoch 106, loss: 2.211931\n",
      "Epoch 107, loss: 2.219092\n",
      "Epoch 108, loss: 2.238311\n",
      "Epoch 109, loss: 2.228967\n",
      "Epoch 110, loss: 2.237705\n",
      "Epoch 111, loss: 2.230655\n",
      "Epoch 112, loss: 2.248091\n",
      "Epoch 113, loss: 2.233591\n",
      "Epoch 114, loss: 2.238257\n",
      "Epoch 115, loss: 2.238884\n",
      "Epoch 116, loss: 2.218576\n",
      "Epoch 117, loss: 2.235894\n",
      "Epoch 118, loss: 2.239583\n",
      "Epoch 119, loss: 2.254862\n",
      "Epoch 120, loss: 2.231742\n",
      "Epoch 121, loss: 2.231301\n",
      "Epoch 122, loss: 2.233645\n",
      "Epoch 123, loss: 2.234351\n",
      "Epoch 124, loss: 2.213354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, loss: 2.239420\n",
      "Epoch 126, loss: 2.207396\n",
      "Epoch 127, loss: 2.205455\n",
      "Epoch 128, loss: 2.220146\n",
      "Epoch 129, loss: 2.232380\n",
      "Epoch 130, loss: 2.229632\n",
      "Epoch 131, loss: 2.245666\n",
      "Epoch 132, loss: 2.232168\n",
      "Epoch 133, loss: 2.212070\n",
      "Epoch 134, loss: 2.214035\n",
      "Epoch 135, loss: 2.219591\n",
      "Epoch 136, loss: 2.213689\n",
      "Epoch 137, loss: 2.231066\n",
      "Epoch 138, loss: 2.214535\n",
      "Epoch 139, loss: 2.227186\n",
      "Epoch 140, loss: 2.222834\n",
      "Epoch 141, loss: 2.214677\n",
      "Epoch 142, loss: 2.229833\n",
      "Epoch 143, loss: 2.227874\n",
      "Epoch 144, loss: 2.200180\n",
      "Epoch 145, loss: 2.223438\n",
      "Epoch 146, loss: 2.205595\n",
      "Epoch 147, loss: 2.230089\n",
      "Epoch 148, loss: 2.206876\n",
      "Epoch 149, loss: 2.225306\n",
      "Epoch 150, loss: 2.203973\n",
      "Epoch 151, loss: 2.230894\n",
      "Epoch 152, loss: 2.213655\n",
      "Epoch 153, loss: 2.213585\n",
      "Epoch 154, loss: 2.226699\n",
      "Epoch 155, loss: 2.195337\n",
      "Epoch 156, loss: 2.212665\n",
      "Epoch 157, loss: 2.212575\n",
      "Epoch 158, loss: 2.225728\n",
      "Epoch 159, loss: 2.200590\n",
      "Epoch 160, loss: 2.227460\n",
      "Epoch 161, loss: 2.210871\n",
      "Epoch 162, loss: 2.228208\n",
      "Epoch 163, loss: 2.207817\n",
      "Epoch 164, loss: 2.206944\n",
      "Epoch 165, loss: 2.206767\n",
      "Epoch 166, loss: 2.193389\n",
      "Epoch 167, loss: 2.206895\n",
      "Epoch 168, loss: 2.234716\n",
      "Epoch 169, loss: 2.198802\n",
      "Epoch 170, loss: 2.194543\n",
      "Epoch 171, loss: 2.208498\n",
      "Epoch 172, loss: 2.222874\n",
      "Epoch 173, loss: 2.203206\n",
      "Epoch 174, loss: 2.213898\n",
      "Epoch 175, loss: 2.222994\n",
      "Epoch 176, loss: 2.201701\n",
      "Epoch 177, loss: 2.220233\n",
      "Epoch 178, loss: 2.220941\n",
      "Epoch 179, loss: 2.197661\n",
      "Epoch 180, loss: 2.209420\n",
      "Epoch 181, loss: 2.199146\n",
      "Epoch 182, loss: 2.214395\n",
      "Epoch 183, loss: 2.224947\n",
      "Epoch 184, loss: 2.219461\n",
      "Epoch 185, loss: 2.227085\n",
      "Epoch 186, loss: 2.211137\n",
      "Epoch 187, loss: 2.193881\n",
      "Epoch 188, loss: 2.206112\n",
      "Epoch 189, loss: 2.183144\n",
      "Epoch 190, loss: 2.196448\n",
      "Epoch 191, loss: 2.186080\n",
      "Epoch 192, loss: 2.192052\n",
      "Epoch 193, loss: 2.203634\n",
      "Epoch 194, loss: 2.201390\n",
      "Epoch 195, loss: 2.214965\n",
      "Epoch 196, loss: 2.191912\n",
      "Epoch 197, loss: 2.217274\n",
      "Epoch 198, loss: 2.184457\n",
      "Epoch 199, loss: 2.189368\n",
      "Epoch 0, loss: 2.302129\n",
      "Epoch 1, loss: 2.300423\n",
      "Epoch 2, loss: 2.297667\n",
      "Epoch 3, loss: 2.298296\n",
      "Epoch 4, loss: 2.297824\n",
      "Epoch 5, loss: 2.296578\n",
      "Epoch 6, loss: 2.293818\n",
      "Epoch 7, loss: 2.294969\n",
      "Epoch 8, loss: 2.294032\n",
      "Epoch 9, loss: 2.292382\n",
      "Epoch 10, loss: 2.287597\n",
      "Epoch 11, loss: 2.294209\n",
      "Epoch 12, loss: 2.293366\n",
      "Epoch 13, loss: 2.291295\n",
      "Epoch 14, loss: 2.286367\n",
      "Epoch 15, loss: 2.291076\n",
      "Epoch 16, loss: 2.285752\n",
      "Epoch 17, loss: 2.290092\n",
      "Epoch 18, loss: 2.286509\n",
      "Epoch 19, loss: 2.282915\n",
      "Epoch 20, loss: 2.287176\n",
      "Epoch 21, loss: 2.287994\n",
      "Epoch 22, loss: 2.279103\n",
      "Epoch 23, loss: 2.283140\n",
      "Epoch 24, loss: 2.280689\n",
      "Epoch 25, loss: 2.282042\n",
      "Epoch 26, loss: 2.281763\n",
      "Epoch 27, loss: 2.278217\n",
      "Epoch 28, loss: 2.284176\n",
      "Epoch 29, loss: 2.275853\n",
      "Epoch 30, loss: 2.280278\n",
      "Epoch 31, loss: 2.274019\n",
      "Epoch 32, loss: 2.272897\n",
      "Epoch 33, loss: 2.274112\n",
      "Epoch 34, loss: 2.282380\n",
      "Epoch 35, loss: 2.270541\n",
      "Epoch 36, loss: 2.277493\n",
      "Epoch 37, loss: 2.265591\n",
      "Epoch 38, loss: 2.273610\n",
      "Epoch 39, loss: 2.263384\n",
      "Epoch 40, loss: 2.263825\n",
      "Epoch 41, loss: 2.273294\n",
      "Epoch 42, loss: 2.265441\n",
      "Epoch 43, loss: 2.271572\n",
      "Epoch 44, loss: 2.267402\n",
      "Epoch 45, loss: 2.276080\n",
      "Epoch 46, loss: 2.263682\n",
      "Epoch 47, loss: 2.264505\n",
      "Epoch 48, loss: 2.266758\n",
      "Epoch 49, loss: 2.264160\n",
      "Epoch 50, loss: 2.262614\n",
      "Epoch 51, loss: 2.265243\n",
      "Epoch 52, loss: 2.256829\n",
      "Epoch 53, loss: 2.268528\n",
      "Epoch 54, loss: 2.263165\n",
      "Epoch 55, loss: 2.257292\n",
      "Epoch 56, loss: 2.271366\n",
      "Epoch 57, loss: 2.249133\n",
      "Epoch 58, loss: 2.255806\n",
      "Epoch 59, loss: 2.257141\n",
      "Epoch 60, loss: 2.261399\n",
      "Epoch 61, loss: 2.254205\n",
      "Epoch 62, loss: 2.258710\n",
      "Epoch 63, loss: 2.251830\n",
      "Epoch 64, loss: 2.246886\n",
      "Epoch 65, loss: 2.257862\n",
      "Epoch 66, loss: 2.253254\n",
      "Epoch 67, loss: 2.265297\n",
      "Epoch 68, loss: 2.247196\n",
      "Epoch 69, loss: 2.254780\n",
      "Epoch 70, loss: 2.244639\n",
      "Epoch 71, loss: 2.248510\n",
      "Epoch 72, loss: 2.264739\n",
      "Epoch 73, loss: 2.260717\n",
      "Epoch 74, loss: 2.256524\n",
      "Epoch 75, loss: 2.248260\n",
      "Epoch 76, loss: 2.264449\n",
      "Epoch 77, loss: 2.244954\n",
      "Epoch 78, loss: 2.256513\n",
      "Epoch 79, loss: 2.247579\n",
      "Epoch 80, loss: 2.247926\n",
      "Epoch 81, loss: 2.237210\n",
      "Epoch 82, loss: 2.241995\n",
      "Epoch 83, loss: 2.258771\n",
      "Epoch 84, loss: 2.232269\n",
      "Epoch 85, loss: 2.250791\n",
      "Epoch 86, loss: 2.236052\n",
      "Epoch 87, loss: 2.260371\n",
      "Epoch 88, loss: 2.240492\n",
      "Epoch 89, loss: 2.237552\n",
      "Epoch 90, loss: 2.236867\n",
      "Epoch 91, loss: 2.240260\n",
      "Epoch 92, loss: 2.234258\n",
      "Epoch 93, loss: 2.246003\n",
      "Epoch 94, loss: 2.246058\n",
      "Epoch 95, loss: 2.238976\n",
      "Epoch 96, loss: 2.253234\n",
      "Epoch 97, loss: 2.241119\n",
      "Epoch 98, loss: 2.233402\n",
      "Epoch 99, loss: 2.229989\n",
      "Epoch 100, loss: 2.232675\n",
      "Epoch 101, loss: 2.241485\n",
      "Epoch 102, loss: 2.222915\n",
      "Epoch 103, loss: 2.238419\n",
      "Epoch 104, loss: 2.231413\n",
      "Epoch 105, loss: 2.258204\n",
      "Epoch 106, loss: 2.248248\n",
      "Epoch 107, loss: 2.228920\n",
      "Epoch 108, loss: 2.231651\n",
      "Epoch 109, loss: 2.225883\n",
      "Epoch 110, loss: 2.233264\n",
      "Epoch 111, loss: 2.235821\n",
      "Epoch 112, loss: 2.211092\n",
      "Epoch 113, loss: 2.220765\n",
      "Epoch 114, loss: 2.232346\n",
      "Epoch 115, loss: 2.220712\n",
      "Epoch 116, loss: 2.238493\n",
      "Epoch 117, loss: 2.211011\n",
      "Epoch 118, loss: 2.222705\n",
      "Epoch 119, loss: 2.247158\n",
      "Epoch 120, loss: 2.227730\n",
      "Epoch 121, loss: 2.231869\n",
      "Epoch 122, loss: 2.240492\n",
      "Epoch 123, loss: 2.227846\n",
      "Epoch 124, loss: 2.239288\n",
      "Epoch 125, loss: 2.233642\n",
      "Epoch 126, loss: 2.229731\n",
      "Epoch 127, loss: 2.232483\n",
      "Epoch 128, loss: 2.200338\n",
      "Epoch 129, loss: 2.245741\n",
      "Epoch 130, loss: 2.231848\n",
      "Epoch 131, loss: 2.228731\n",
      "Epoch 132, loss: 2.219027\n",
      "Epoch 133, loss: 2.229266\n",
      "Epoch 134, loss: 2.212374\n",
      "Epoch 135, loss: 2.228607\n",
      "Epoch 136, loss: 2.214200\n",
      "Epoch 137, loss: 2.200201\n",
      "Epoch 138, loss: 2.212400\n",
      "Epoch 139, loss: 2.214711\n",
      "Epoch 140, loss: 2.226099\n",
      "Epoch 141, loss: 2.216612\n",
      "Epoch 142, loss: 2.233578\n",
      "Epoch 143, loss: 2.216056\n",
      "Epoch 144, loss: 2.220381\n",
      "Epoch 145, loss: 2.252339\n",
      "Epoch 146, loss: 2.234932\n",
      "Epoch 147, loss: 2.212430\n",
      "Epoch 148, loss: 2.205642\n",
      "Epoch 149, loss: 2.215859\n",
      "Epoch 150, loss: 2.230767\n",
      "Epoch 151, loss: 2.220905\n",
      "Epoch 152, loss: 2.240320\n",
      "Epoch 153, loss: 2.213532\n",
      "Epoch 154, loss: 2.185191\n",
      "Epoch 155, loss: 2.208406\n",
      "Epoch 156, loss: 2.215571\n",
      "Epoch 157, loss: 2.233666\n",
      "Epoch 158, loss: 2.228945\n",
      "Epoch 159, loss: 2.224019\n",
      "Epoch 160, loss: 2.200211\n",
      "Epoch 161, loss: 2.211523\n",
      "Epoch 162, loss: 2.220928\n",
      "Epoch 163, loss: 2.180413\n",
      "Epoch 164, loss: 2.183887\n",
      "Epoch 165, loss: 2.215811\n",
      "Epoch 166, loss: 2.187771\n",
      "Epoch 167, loss: 2.216310\n",
      "Epoch 168, loss: 2.223218\n",
      "Epoch 169, loss: 2.213807\n",
      "Epoch 170, loss: 2.204597\n",
      "Epoch 171, loss: 2.207799\n",
      "Epoch 172, loss: 2.212272\n",
      "Epoch 173, loss: 2.190164\n",
      "Epoch 174, loss: 2.213684\n",
      "Epoch 175, loss: 2.217286\n",
      "Epoch 176, loss: 2.175573\n",
      "Epoch 177, loss: 2.190128\n",
      "Epoch 178, loss: 2.215714\n",
      "Epoch 179, loss: 2.199233\n",
      "Epoch 180, loss: 2.213456\n",
      "Epoch 181, loss: 2.198477\n",
      "Epoch 182, loss: 2.226415\n",
      "Epoch 183, loss: 2.189646\n",
      "Epoch 184, loss: 2.203420\n",
      "Epoch 185, loss: 2.200878\n",
      "Epoch 186, loss: 2.205884\n",
      "Epoch 187, loss: 2.173906\n",
      "Epoch 188, loss: 2.160994\n",
      "Epoch 189, loss: 2.201256\n",
      "Epoch 190, loss: 2.220697\n",
      "Epoch 191, loss: 2.185583\n",
      "Epoch 192, loss: 2.202771\n",
      "Epoch 193, loss: 2.217678\n",
      "Epoch 194, loss: 2.207147\n",
      "Epoch 195, loss: 2.215324\n",
      "Epoch 196, loss: 2.206569\n",
      "Epoch 197, loss: 2.214488\n",
      "Epoch 198, loss: 2.211831\n",
      "Epoch 199, loss: 2.191940\n",
      "Epoch 0, loss: 2.301768\n",
      "Epoch 1, loss: 2.303202\n",
      "Epoch 2, loss: 2.302646\n",
      "Epoch 3, loss: 2.302404\n",
      "Epoch 4, loss: 2.303433\n",
      "Epoch 5, loss: 2.302129\n",
      "Epoch 6, loss: 2.300904\n",
      "Epoch 7, loss: 2.301225\n",
      "Epoch 8, loss: 2.302707\n",
      "Epoch 9, loss: 2.301979\n",
      "Epoch 10, loss: 2.300716\n",
      "Epoch 11, loss: 2.300897\n",
      "Epoch 12, loss: 2.300401\n",
      "Epoch 13, loss: 2.300507\n",
      "Epoch 14, loss: 2.300763\n",
      "Epoch 15, loss: 2.300794\n",
      "Epoch 16, loss: 2.300655\n",
      "Epoch 17, loss: 2.301184\n",
      "Epoch 18, loss: 2.300415\n",
      "Epoch 19, loss: 2.300183\n",
      "Epoch 20, loss: 2.300324\n",
      "Epoch 21, loss: 2.301015\n",
      "Epoch 22, loss: 2.300541\n",
      "Epoch 23, loss: 2.299770\n",
      "Epoch 24, loss: 2.300258\n",
      "Epoch 25, loss: 2.298641\n",
      "Epoch 26, loss: 2.299767\n",
      "Epoch 27, loss: 2.299556\n",
      "Epoch 28, loss: 2.299493\n",
      "Epoch 29, loss: 2.299726\n",
      "Epoch 30, loss: 2.298489\n",
      "Epoch 31, loss: 2.299773\n",
      "Epoch 32, loss: 2.299561\n",
      "Epoch 33, loss: 2.300018\n",
      "Epoch 34, loss: 2.298884\n",
      "Epoch 35, loss: 2.298823\n",
      "Epoch 36, loss: 2.297857\n",
      "Epoch 37, loss: 2.299124\n",
      "Epoch 38, loss: 2.298756\n",
      "Epoch 39, loss: 2.298262\n",
      "Epoch 40, loss: 2.297630\n",
      "Epoch 41, loss: 2.300057\n",
      "Epoch 42, loss: 2.296780\n",
      "Epoch 43, loss: 2.299416\n",
      "Epoch 44, loss: 2.297972\n",
      "Epoch 45, loss: 2.298798\n",
      "Epoch 46, loss: 2.297125\n",
      "Epoch 47, loss: 2.300320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, loss: 2.296503\n",
      "Epoch 49, loss: 2.297206\n",
      "Epoch 50, loss: 2.297678\n",
      "Epoch 51, loss: 2.295386\n",
      "Epoch 52, loss: 2.298823\n",
      "Epoch 53, loss: 2.299984\n",
      "Epoch 54, loss: 2.298266\n",
      "Epoch 55, loss: 2.298445\n",
      "Epoch 56, loss: 2.296018\n",
      "Epoch 57, loss: 2.293955\n",
      "Epoch 58, loss: 2.293544\n",
      "Epoch 59, loss: 2.297933\n",
      "Epoch 60, loss: 2.296878\n",
      "Epoch 61, loss: 2.299252\n",
      "Epoch 62, loss: 2.296704\n",
      "Epoch 63, loss: 2.297080\n",
      "Epoch 64, loss: 2.296752\n",
      "Epoch 65, loss: 2.296030\n",
      "Epoch 66, loss: 2.296714\n",
      "Epoch 67, loss: 2.292626\n",
      "Epoch 68, loss: 2.298876\n",
      "Epoch 69, loss: 2.294463\n",
      "Epoch 70, loss: 2.293619\n",
      "Epoch 71, loss: 2.294193\n",
      "Epoch 72, loss: 2.296057\n",
      "Epoch 73, loss: 2.294224\n",
      "Epoch 74, loss: 2.296006\n",
      "Epoch 75, loss: 2.297610\n",
      "Epoch 76, loss: 2.294625\n",
      "Epoch 77, loss: 2.296302\n",
      "Epoch 78, loss: 2.293777\n",
      "Epoch 79, loss: 2.295422\n",
      "Epoch 80, loss: 2.293928\n",
      "Epoch 81, loss: 2.294437\n",
      "Epoch 82, loss: 2.295817\n",
      "Epoch 83, loss: 2.294605\n",
      "Epoch 84, loss: 2.294844\n",
      "Epoch 85, loss: 2.293008\n",
      "Epoch 86, loss: 2.294338\n",
      "Epoch 87, loss: 2.292312\n",
      "Epoch 88, loss: 2.290189\n",
      "Epoch 89, loss: 2.292825\n",
      "Epoch 90, loss: 2.292054\n",
      "Epoch 91, loss: 2.294738\n",
      "Epoch 92, loss: 2.295472\n",
      "Epoch 93, loss: 2.291825\n",
      "Epoch 94, loss: 2.289544\n",
      "Epoch 95, loss: 2.297132\n",
      "Epoch 96, loss: 2.294282\n",
      "Epoch 97, loss: 2.297401\n",
      "Epoch 98, loss: 2.292837\n",
      "Epoch 99, loss: 2.292070\n",
      "Epoch 100, loss: 2.295110\n",
      "Epoch 101, loss: 2.290998\n",
      "Epoch 102, loss: 2.296297\n",
      "Epoch 103, loss: 2.291156\n",
      "Epoch 104, loss: 2.295694\n",
      "Epoch 105, loss: 2.293047\n",
      "Epoch 106, loss: 2.287786\n",
      "Epoch 107, loss: 2.291837\n",
      "Epoch 108, loss: 2.293874\n",
      "Epoch 109, loss: 2.293989\n",
      "Epoch 110, loss: 2.288240\n",
      "Epoch 111, loss: 2.292050\n",
      "Epoch 112, loss: 2.290021\n",
      "Epoch 113, loss: 2.292534\n",
      "Epoch 114, loss: 2.296709\n",
      "Epoch 115, loss: 2.295221\n",
      "Epoch 116, loss: 2.294517\n",
      "Epoch 117, loss: 2.289612\n",
      "Epoch 118, loss: 2.289395\n",
      "Epoch 119, loss: 2.290062\n",
      "Epoch 120, loss: 2.290831\n",
      "Epoch 121, loss: 2.289117\n",
      "Epoch 122, loss: 2.291125\n",
      "Epoch 123, loss: 2.293774\n",
      "Epoch 124, loss: 2.292043\n",
      "Epoch 125, loss: 2.289050\n",
      "Epoch 126, loss: 2.290774\n",
      "Epoch 127, loss: 2.293200\n",
      "Epoch 128, loss: 2.291439\n",
      "Epoch 129, loss: 2.297280\n",
      "Epoch 130, loss: 2.288472\n",
      "Epoch 131, loss: 2.288910\n",
      "Epoch 132, loss: 2.288482\n",
      "Epoch 133, loss: 2.291179\n",
      "Epoch 134, loss: 2.291846\n",
      "Epoch 135, loss: 2.290956\n",
      "Epoch 136, loss: 2.289554\n",
      "Epoch 137, loss: 2.290442\n",
      "Epoch 138, loss: 2.288175\n",
      "Epoch 139, loss: 2.290609\n",
      "Epoch 140, loss: 2.289157\n",
      "Epoch 141, loss: 2.286643\n",
      "Epoch 142, loss: 2.284647\n",
      "Epoch 143, loss: 2.288590\n",
      "Epoch 144, loss: 2.294317\n",
      "Epoch 145, loss: 2.292301\n",
      "Epoch 146, loss: 2.292526\n",
      "Epoch 147, loss: 2.287678\n",
      "Epoch 148, loss: 2.288296\n",
      "Epoch 149, loss: 2.287669\n",
      "Epoch 150, loss: 2.289377\n",
      "Epoch 151, loss: 2.289563\n",
      "Epoch 152, loss: 2.286098\n",
      "Epoch 153, loss: 2.285886\n",
      "Epoch 154, loss: 2.286502\n",
      "Epoch 155, loss: 2.289655\n",
      "Epoch 156, loss: 2.289064\n",
      "Epoch 157, loss: 2.286336\n",
      "Epoch 158, loss: 2.288684\n",
      "Epoch 159, loss: 2.289575\n",
      "Epoch 160, loss: 2.284893\n",
      "Epoch 161, loss: 2.290008\n",
      "Epoch 162, loss: 2.287396\n",
      "Epoch 163, loss: 2.289044\n",
      "Epoch 164, loss: 2.286842\n",
      "Epoch 165, loss: 2.282991\n",
      "Epoch 166, loss: 2.295489\n",
      "Epoch 167, loss: 2.291475\n",
      "Epoch 168, loss: 2.289270\n",
      "Epoch 169, loss: 2.288535\n",
      "Epoch 170, loss: 2.288244\n",
      "Epoch 171, loss: 2.284457\n",
      "Epoch 172, loss: 2.284351\n",
      "Epoch 173, loss: 2.287678\n",
      "Epoch 174, loss: 2.291125\n",
      "Epoch 175, loss: 2.283330\n",
      "Epoch 176, loss: 2.289206\n",
      "Epoch 177, loss: 2.285117\n",
      "Epoch 178, loss: 2.282336\n",
      "Epoch 179, loss: 2.284365\n",
      "Epoch 180, loss: 2.283475\n",
      "Epoch 181, loss: 2.285052\n",
      "Epoch 182, loss: 2.282081\n",
      "Epoch 183, loss: 2.292440\n",
      "Epoch 184, loss: 2.291659\n",
      "Epoch 185, loss: 2.288653\n",
      "Epoch 186, loss: 2.287853\n",
      "Epoch 187, loss: 2.292818\n",
      "Epoch 188, loss: 2.288532\n",
      "Epoch 189, loss: 2.284549\n",
      "Epoch 190, loss: 2.281103\n",
      "Epoch 191, loss: 2.285795\n",
      "Epoch 192, loss: 2.285805\n",
      "Epoch 193, loss: 2.287722\n",
      "Epoch 194, loss: 2.290665\n",
      "Epoch 195, loss: 2.287511\n",
      "Epoch 196, loss: 2.287010\n",
      "Epoch 197, loss: 2.284331\n",
      "Epoch 198, loss: 2.286515\n",
      "Epoch 199, loss: 2.283061\n",
      "Epoch 0, loss: 2.301771\n",
      "Epoch 1, loss: 2.301801\n",
      "Epoch 2, loss: 2.302836\n",
      "Epoch 3, loss: 2.302236\n",
      "Epoch 4, loss: 2.302114\n",
      "Epoch 5, loss: 2.301729\n",
      "Epoch 6, loss: 2.301990\n",
      "Epoch 7, loss: 2.301763\n",
      "Epoch 8, loss: 2.302154\n",
      "Epoch 9, loss: 2.302165\n",
      "Epoch 10, loss: 2.301884\n",
      "Epoch 11, loss: 2.301087\n",
      "Epoch 12, loss: 2.301613\n",
      "Epoch 13, loss: 2.301587\n",
      "Epoch 14, loss: 2.300639\n",
      "Epoch 15, loss: 2.301049\n",
      "Epoch 16, loss: 2.301050\n",
      "Epoch 17, loss: 2.300079\n",
      "Epoch 18, loss: 2.299898\n",
      "Epoch 19, loss: 2.298835\n",
      "Epoch 20, loss: 2.300349\n",
      "Epoch 21, loss: 2.300783\n",
      "Epoch 22, loss: 2.300154\n",
      "Epoch 23, loss: 2.300723\n",
      "Epoch 24, loss: 2.300297\n",
      "Epoch 25, loss: 2.299757\n",
      "Epoch 26, loss: 2.300230\n",
      "Epoch 27, loss: 2.299682\n",
      "Epoch 28, loss: 2.300431\n",
      "Epoch 29, loss: 2.299867\n",
      "Epoch 30, loss: 2.298177\n",
      "Epoch 31, loss: 2.298986\n",
      "Epoch 32, loss: 2.299464\n",
      "Epoch 33, loss: 2.298929\n",
      "Epoch 34, loss: 2.299550\n",
      "Epoch 35, loss: 2.300027\n",
      "Epoch 36, loss: 2.299648\n",
      "Epoch 37, loss: 2.298236\n",
      "Epoch 38, loss: 2.300592\n",
      "Epoch 39, loss: 2.299407\n",
      "Epoch 40, loss: 2.296185\n",
      "Epoch 41, loss: 2.298028\n",
      "Epoch 42, loss: 2.298340\n",
      "Epoch 43, loss: 2.296797\n",
      "Epoch 44, loss: 2.299682\n",
      "Epoch 45, loss: 2.300388\n",
      "Epoch 46, loss: 2.298078\n",
      "Epoch 47, loss: 2.296636\n",
      "Epoch 48, loss: 2.298099\n",
      "Epoch 49, loss: 2.298134\n",
      "Epoch 50, loss: 2.298220\n",
      "Epoch 51, loss: 2.299224\n",
      "Epoch 52, loss: 2.299116\n",
      "Epoch 53, loss: 2.294596\n",
      "Epoch 54, loss: 2.296571\n",
      "Epoch 55, loss: 2.296175\n",
      "Epoch 56, loss: 2.299358\n",
      "Epoch 57, loss: 2.295367\n",
      "Epoch 58, loss: 2.295575\n",
      "Epoch 59, loss: 2.297269\n",
      "Epoch 60, loss: 2.297304\n",
      "Epoch 61, loss: 2.294492\n",
      "Epoch 62, loss: 2.296729\n",
      "Epoch 63, loss: 2.297252\n",
      "Epoch 64, loss: 2.297217\n",
      "Epoch 65, loss: 2.290753\n",
      "Epoch 66, loss: 2.297430\n",
      "Epoch 67, loss: 2.294612\n",
      "Epoch 68, loss: 2.296475\n",
      "Epoch 69, loss: 2.295757\n",
      "Epoch 70, loss: 2.299547\n",
      "Epoch 71, loss: 2.294859\n",
      "Epoch 72, loss: 2.295719\n",
      "Epoch 73, loss: 2.294646\n",
      "Epoch 74, loss: 2.294914\n",
      "Epoch 75, loss: 2.299166\n",
      "Epoch 76, loss: 2.297799\n",
      "Epoch 77, loss: 2.295884\n",
      "Epoch 78, loss: 2.294490\n",
      "Epoch 79, loss: 2.292773\n",
      "Epoch 80, loss: 2.294374\n",
      "Epoch 81, loss: 2.293687\n",
      "Epoch 82, loss: 2.293659\n",
      "Epoch 83, loss: 2.293915\n",
      "Epoch 84, loss: 2.296139\n",
      "Epoch 85, loss: 2.296986\n",
      "Epoch 86, loss: 2.297947\n",
      "Epoch 87, loss: 2.292514\n",
      "Epoch 88, loss: 2.294483\n",
      "Epoch 89, loss: 2.293741\n",
      "Epoch 90, loss: 2.295567\n",
      "Epoch 91, loss: 2.292811\n",
      "Epoch 92, loss: 2.293770\n",
      "Epoch 93, loss: 2.293435\n",
      "Epoch 94, loss: 2.294769\n",
      "Epoch 95, loss: 2.297693\n",
      "Epoch 96, loss: 2.288178\n",
      "Epoch 97, loss: 2.289431\n",
      "Epoch 98, loss: 2.293144\n",
      "Epoch 99, loss: 2.293767\n",
      "Epoch 100, loss: 2.295893\n",
      "Epoch 101, loss: 2.290133\n",
      "Epoch 102, loss: 2.293172\n",
      "Epoch 103, loss: 2.294737\n",
      "Epoch 104, loss: 2.293102\n",
      "Epoch 105, loss: 2.298010\n",
      "Epoch 106, loss: 2.289758\n",
      "Epoch 107, loss: 2.292678\n",
      "Epoch 108, loss: 2.292527\n",
      "Epoch 109, loss: 2.293150\n",
      "Epoch 110, loss: 2.292302\n",
      "Epoch 111, loss: 2.292713\n",
      "Epoch 112, loss: 2.288106\n",
      "Epoch 113, loss: 2.295037\n",
      "Epoch 114, loss: 2.290170\n",
      "Epoch 115, loss: 2.292152\n",
      "Epoch 116, loss: 2.289716\n",
      "Epoch 117, loss: 2.290346\n",
      "Epoch 118, loss: 2.296326\n",
      "Epoch 119, loss: 2.291147\n",
      "Epoch 120, loss: 2.290247\n",
      "Epoch 121, loss: 2.290155\n",
      "Epoch 122, loss: 2.290092\n",
      "Epoch 123, loss: 2.294393\n",
      "Epoch 124, loss: 2.293103\n",
      "Epoch 125, loss: 2.288773\n",
      "Epoch 126, loss: 2.288387\n",
      "Epoch 127, loss: 2.291905\n",
      "Epoch 128, loss: 2.286013\n",
      "Epoch 129, loss: 2.292267\n",
      "Epoch 130, loss: 2.291583\n",
      "Epoch 131, loss: 2.289003\n",
      "Epoch 132, loss: 2.291311\n",
      "Epoch 133, loss: 2.293874\n",
      "Epoch 134, loss: 2.288034\n",
      "Epoch 135, loss: 2.287981\n",
      "Epoch 136, loss: 2.290460\n",
      "Epoch 137, loss: 2.290029\n",
      "Epoch 138, loss: 2.288703\n",
      "Epoch 139, loss: 2.285689\n",
      "Epoch 140, loss: 2.292271\n",
      "Epoch 141, loss: 2.289243\n",
      "Epoch 142, loss: 2.290327\n",
      "Epoch 143, loss: 2.291907\n",
      "Epoch 144, loss: 2.294281\n",
      "Epoch 145, loss: 2.290237\n",
      "Epoch 146, loss: 2.292154\n",
      "Epoch 147, loss: 2.288610\n",
      "Epoch 148, loss: 2.290894\n",
      "Epoch 149, loss: 2.291323\n",
      "Epoch 150, loss: 2.292885\n",
      "Epoch 151, loss: 2.288806\n",
      "Epoch 152, loss: 2.292627\n",
      "Epoch 153, loss: 2.285790\n",
      "Epoch 154, loss: 2.288536\n",
      "Epoch 155, loss: 2.284126\n",
      "Epoch 156, loss: 2.289910\n",
      "Epoch 157, loss: 2.288666\n",
      "Epoch 158, loss: 2.283433\n",
      "Epoch 159, loss: 2.288280\n",
      "Epoch 160, loss: 2.284458\n",
      "Epoch 161, loss: 2.288648\n",
      "Epoch 162, loss: 2.287604\n",
      "Epoch 163, loss: 2.289654\n",
      "Epoch 164, loss: 2.283753\n",
      "Epoch 165, loss: 2.287051\n",
      "Epoch 166, loss: 2.286715\n",
      "Epoch 167, loss: 2.291569\n",
      "Epoch 168, loss: 2.289339\n",
      "Epoch 169, loss: 2.287323\n",
      "Epoch 170, loss: 2.285975\n",
      "Epoch 171, loss: 2.282980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, loss: 2.291652\n",
      "Epoch 173, loss: 2.279160\n",
      "Epoch 174, loss: 2.287349\n",
      "Epoch 175, loss: 2.286837\n",
      "Epoch 176, loss: 2.289794\n",
      "Epoch 177, loss: 2.284022\n",
      "Epoch 178, loss: 2.287745\n",
      "Epoch 179, loss: 2.282112\n",
      "Epoch 180, loss: 2.287697\n",
      "Epoch 181, loss: 2.285027\n",
      "Epoch 182, loss: 2.284646\n",
      "Epoch 183, loss: 2.287859\n",
      "Epoch 184, loss: 2.284053\n",
      "Epoch 185, loss: 2.286864\n",
      "Epoch 186, loss: 2.286501\n",
      "Epoch 187, loss: 2.285809\n",
      "Epoch 188, loss: 2.291108\n",
      "Epoch 189, loss: 2.281090\n",
      "Epoch 190, loss: 2.281911\n",
      "Epoch 191, loss: 2.285369\n",
      "Epoch 192, loss: 2.282304\n",
      "Epoch 193, loss: 2.284124\n",
      "Epoch 194, loss: 2.281656\n",
      "Epoch 195, loss: 2.286627\n",
      "Epoch 196, loss: 2.291401\n",
      "Epoch 197, loss: 2.285764\n",
      "Epoch 198, loss: 2.289450\n",
      "Epoch 199, loss: 2.289842\n",
      "Epoch 0, loss: 2.302848\n",
      "Epoch 1, loss: 2.302363\n",
      "Epoch 2, loss: 2.302331\n",
      "Epoch 3, loss: 2.301634\n",
      "Epoch 4, loss: 2.301261\n",
      "Epoch 5, loss: 2.302085\n",
      "Epoch 6, loss: 2.302478\n",
      "Epoch 7, loss: 2.300776\n",
      "Epoch 8, loss: 2.300690\n",
      "Epoch 9, loss: 2.301700\n",
      "Epoch 10, loss: 2.302105\n",
      "Epoch 11, loss: 2.301545\n",
      "Epoch 12, loss: 2.301523\n",
      "Epoch 13, loss: 2.301330\n",
      "Epoch 14, loss: 2.302018\n",
      "Epoch 15, loss: 2.300394\n",
      "Epoch 16, loss: 2.300759\n",
      "Epoch 17, loss: 2.300536\n",
      "Epoch 18, loss: 2.301423\n",
      "Epoch 19, loss: 2.299925\n",
      "Epoch 20, loss: 2.299701\n",
      "Epoch 21, loss: 2.299105\n",
      "Epoch 22, loss: 2.301018\n",
      "Epoch 23, loss: 2.300740\n",
      "Epoch 24, loss: 2.299256\n",
      "Epoch 25, loss: 2.299952\n",
      "Epoch 26, loss: 2.299323\n",
      "Epoch 27, loss: 2.299424\n",
      "Epoch 28, loss: 2.300726\n",
      "Epoch 29, loss: 2.298864\n",
      "Epoch 30, loss: 2.300104\n",
      "Epoch 31, loss: 2.299787\n",
      "Epoch 32, loss: 2.299663\n",
      "Epoch 33, loss: 2.298825\n",
      "Epoch 34, loss: 2.299859\n",
      "Epoch 35, loss: 2.298456\n",
      "Epoch 36, loss: 2.301551\n",
      "Epoch 37, loss: 2.299270\n",
      "Epoch 38, loss: 2.298937\n",
      "Epoch 39, loss: 2.298990\n",
      "Epoch 40, loss: 2.298464\n",
      "Epoch 41, loss: 2.298814\n",
      "Epoch 42, loss: 2.298182\n",
      "Epoch 43, loss: 2.297778\n",
      "Epoch 44, loss: 2.298529\n",
      "Epoch 45, loss: 2.296738\n",
      "Epoch 46, loss: 2.300233\n",
      "Epoch 47, loss: 2.297989\n",
      "Epoch 48, loss: 2.299970\n",
      "Epoch 49, loss: 2.297450\n",
      "Epoch 50, loss: 2.297879\n",
      "Epoch 51, loss: 2.297603\n",
      "Epoch 52, loss: 2.298263\n",
      "Epoch 53, loss: 2.295780\n",
      "Epoch 54, loss: 2.296805\n",
      "Epoch 55, loss: 2.298900\n",
      "Epoch 56, loss: 2.294762\n",
      "Epoch 57, loss: 2.296570\n",
      "Epoch 58, loss: 2.298112\n",
      "Epoch 59, loss: 2.297004\n",
      "Epoch 60, loss: 2.295216\n",
      "Epoch 61, loss: 2.294887\n",
      "Epoch 62, loss: 2.295532\n",
      "Epoch 63, loss: 2.297541\n",
      "Epoch 64, loss: 2.293583\n",
      "Epoch 65, loss: 2.298998\n",
      "Epoch 66, loss: 2.295859\n",
      "Epoch 67, loss: 2.298288\n",
      "Epoch 68, loss: 2.292933\n",
      "Epoch 69, loss: 2.296862\n",
      "Epoch 70, loss: 2.294575\n",
      "Epoch 71, loss: 2.296397\n",
      "Epoch 72, loss: 2.294892\n",
      "Epoch 73, loss: 2.298685\n",
      "Epoch 74, loss: 2.294567\n",
      "Epoch 75, loss: 2.296488\n",
      "Epoch 76, loss: 2.295849\n",
      "Epoch 77, loss: 2.294594\n",
      "Epoch 78, loss: 2.294124\n",
      "Epoch 79, loss: 2.292552\n",
      "Epoch 80, loss: 2.293675\n",
      "Epoch 81, loss: 2.296621\n",
      "Epoch 82, loss: 2.295915\n",
      "Epoch 83, loss: 2.295305\n",
      "Epoch 84, loss: 2.293586\n",
      "Epoch 85, loss: 2.295619\n",
      "Epoch 86, loss: 2.291841\n",
      "Epoch 87, loss: 2.292239\n",
      "Epoch 88, loss: 2.290684\n",
      "Epoch 89, loss: 2.293103\n",
      "Epoch 90, loss: 2.296765\n",
      "Epoch 91, loss: 2.294367\n",
      "Epoch 92, loss: 2.296287\n",
      "Epoch 93, loss: 2.294887\n",
      "Epoch 94, loss: 2.291502\n",
      "Epoch 95, loss: 2.294612\n",
      "Epoch 96, loss: 2.294703\n",
      "Epoch 97, loss: 2.298774\n",
      "Epoch 98, loss: 2.296022\n",
      "Epoch 99, loss: 2.291963\n",
      "Epoch 100, loss: 2.293770\n",
      "Epoch 101, loss: 2.294066\n",
      "Epoch 102, loss: 2.291205\n",
      "Epoch 103, loss: 2.292831\n",
      "Epoch 104, loss: 2.295498\n",
      "Epoch 105, loss: 2.290582\n",
      "Epoch 106, loss: 2.295828\n",
      "Epoch 107, loss: 2.296568\n",
      "Epoch 108, loss: 2.292349\n",
      "Epoch 109, loss: 2.290034\n",
      "Epoch 110, loss: 2.288103\n",
      "Epoch 111, loss: 2.291917\n",
      "Epoch 112, loss: 2.295293\n",
      "Epoch 113, loss: 2.293358\n",
      "Epoch 114, loss: 2.293525\n",
      "Epoch 115, loss: 2.291149\n",
      "Epoch 116, loss: 2.291769\n",
      "Epoch 117, loss: 2.293633\n",
      "Epoch 118, loss: 2.294477\n",
      "Epoch 119, loss: 2.291958\n",
      "Epoch 120, loss: 2.293342\n",
      "Epoch 121, loss: 2.292307\n",
      "Epoch 122, loss: 2.288107\n",
      "Epoch 123, loss: 2.291391\n",
      "Epoch 124, loss: 2.290675\n",
      "Epoch 125, loss: 2.297625\n",
      "Epoch 126, loss: 2.284975\n",
      "Epoch 127, loss: 2.292975\n",
      "Epoch 128, loss: 2.295628\n",
      "Epoch 129, loss: 2.288985\n",
      "Epoch 130, loss: 2.286358\n",
      "Epoch 131, loss: 2.290820\n",
      "Epoch 132, loss: 2.290022\n",
      "Epoch 133, loss: 2.289443\n",
      "Epoch 134, loss: 2.294438\n",
      "Epoch 135, loss: 2.291419\n",
      "Epoch 136, loss: 2.293466\n",
      "Epoch 137, loss: 2.289945\n",
      "Epoch 138, loss: 2.291152\n",
      "Epoch 139, loss: 2.292801\n",
      "Epoch 140, loss: 2.290344\n",
      "Epoch 141, loss: 2.290861\n",
      "Epoch 142, loss: 2.290068\n",
      "Epoch 143, loss: 2.293475\n",
      "Epoch 144, loss: 2.289236\n",
      "Epoch 145, loss: 2.291304\n",
      "Epoch 146, loss: 2.290402\n",
      "Epoch 147, loss: 2.286446\n",
      "Epoch 148, loss: 2.288848\n",
      "Epoch 149, loss: 2.288777\n",
      "Epoch 150, loss: 2.291477\n",
      "Epoch 151, loss: 2.285855\n",
      "Epoch 152, loss: 2.287389\n",
      "Epoch 153, loss: 2.291269\n",
      "Epoch 154, loss: 2.284541\n",
      "Epoch 155, loss: 2.287171\n",
      "Epoch 156, loss: 2.289078\n",
      "Epoch 157, loss: 2.287364\n",
      "Epoch 158, loss: 2.288763\n",
      "Epoch 159, loss: 2.287981\n",
      "Epoch 160, loss: 2.288023\n",
      "Epoch 161, loss: 2.292352\n",
      "Epoch 162, loss: 2.288241\n",
      "Epoch 163, loss: 2.286306\n",
      "Epoch 164, loss: 2.291034\n",
      "Epoch 165, loss: 2.284612\n",
      "Epoch 166, loss: 2.291189\n",
      "Epoch 167, loss: 2.284186\n",
      "Epoch 168, loss: 2.285489\n",
      "Epoch 169, loss: 2.285284\n",
      "Epoch 170, loss: 2.285561\n",
      "Epoch 171, loss: 2.286339\n",
      "Epoch 172, loss: 2.289177\n",
      "Epoch 173, loss: 2.288402\n",
      "Epoch 174, loss: 2.287816\n",
      "Epoch 175, loss: 2.287579\n",
      "Epoch 176, loss: 2.284685\n",
      "Epoch 177, loss: 2.282518\n",
      "Epoch 178, loss: 2.290574\n",
      "Epoch 179, loss: 2.292186\n",
      "Epoch 180, loss: 2.283190\n",
      "Epoch 181, loss: 2.288784\n",
      "Epoch 182, loss: 2.285091\n",
      "Epoch 183, loss: 2.286017\n",
      "Epoch 184, loss: 2.287550\n",
      "Epoch 185, loss: 2.289593\n",
      "Epoch 186, loss: 2.282529\n",
      "Epoch 187, loss: 2.284871\n",
      "Epoch 188, loss: 2.287857\n",
      "Epoch 189, loss: 2.283141\n",
      "Epoch 190, loss: 2.287441\n",
      "Epoch 191, loss: 2.282360\n",
      "Epoch 192, loss: 2.289572\n",
      "Epoch 193, loss: 2.284836\n",
      "Epoch 194, loss: 2.285441\n",
      "Epoch 195, loss: 2.285944\n",
      "Epoch 196, loss: 2.283469\n",
      "Epoch 197, loss: 2.287642\n",
      "Epoch 198, loss: 2.286929\n",
      "Epoch 199, loss: 2.286377\n",
      "Epoch 0, loss: 2.301888\n",
      "Epoch 1, loss: 2.302803\n",
      "Epoch 2, loss: 2.302804\n",
      "Epoch 3, loss: 2.302705\n",
      "Epoch 4, loss: 2.302649\n",
      "Epoch 5, loss: 2.303385\n",
      "Epoch 6, loss: 2.301822\n",
      "Epoch 7, loss: 2.303277\n",
      "Epoch 8, loss: 2.302536\n",
      "Epoch 9, loss: 2.302537\n",
      "Epoch 10, loss: 2.301641\n",
      "Epoch 11, loss: 2.302659\n",
      "Epoch 12, loss: 2.302785\n",
      "Epoch 13, loss: 2.302638\n",
      "Epoch 14, loss: 2.302911\n",
      "Epoch 15, loss: 2.301443\n",
      "Epoch 16, loss: 2.303244\n",
      "Epoch 17, loss: 2.303408\n",
      "Epoch 18, loss: 2.303059\n",
      "Epoch 19, loss: 2.302554\n",
      "Epoch 20, loss: 2.302286\n",
      "Epoch 21, loss: 2.303241\n",
      "Epoch 22, loss: 2.301972\n",
      "Epoch 23, loss: 2.302546\n",
      "Epoch 24, loss: 2.302033\n",
      "Epoch 25, loss: 2.302405\n",
      "Epoch 26, loss: 2.302516\n",
      "Epoch 27, loss: 2.302586\n",
      "Epoch 28, loss: 2.302126\n",
      "Epoch 29, loss: 2.302077\n",
      "Epoch 30, loss: 2.301932\n",
      "Epoch 31, loss: 2.302677\n",
      "Epoch 32, loss: 2.303515\n",
      "Epoch 33, loss: 2.302440\n",
      "Epoch 34, loss: 2.303137\n",
      "Epoch 35, loss: 2.302017\n",
      "Epoch 36, loss: 2.301864\n",
      "Epoch 37, loss: 2.302775\n",
      "Epoch 38, loss: 2.302884\n",
      "Epoch 39, loss: 2.301028\n",
      "Epoch 40, loss: 2.302069\n",
      "Epoch 41, loss: 2.301726\n",
      "Epoch 42, loss: 2.301906\n",
      "Epoch 43, loss: 2.302719\n",
      "Epoch 44, loss: 2.302164\n",
      "Epoch 45, loss: 2.301461\n",
      "Epoch 46, loss: 2.301460\n",
      "Epoch 47, loss: 2.302893\n",
      "Epoch 48, loss: 2.301526\n",
      "Epoch 49, loss: 2.302445\n",
      "Epoch 50, loss: 2.302178\n",
      "Epoch 51, loss: 2.301767\n",
      "Epoch 52, loss: 2.301269\n",
      "Epoch 53, loss: 2.303138\n",
      "Epoch 54, loss: 2.301491\n",
      "Epoch 55, loss: 2.300768\n",
      "Epoch 56, loss: 2.301981\n",
      "Epoch 57, loss: 2.301420\n",
      "Epoch 58, loss: 2.302664\n",
      "Epoch 59, loss: 2.302935\n",
      "Epoch 60, loss: 2.301230\n",
      "Epoch 61, loss: 2.301522\n",
      "Epoch 62, loss: 2.302775\n",
      "Epoch 63, loss: 2.302581\n",
      "Epoch 64, loss: 2.301936\n",
      "Epoch 65, loss: 2.301091\n",
      "Epoch 66, loss: 2.302935\n",
      "Epoch 67, loss: 2.303007\n",
      "Epoch 68, loss: 2.301400\n",
      "Epoch 69, loss: 2.302041\n",
      "Epoch 70, loss: 2.302342\n",
      "Epoch 71, loss: 2.301488\n",
      "Epoch 72, loss: 2.301926\n",
      "Epoch 73, loss: 2.302400\n",
      "Epoch 74, loss: 2.301022\n",
      "Epoch 75, loss: 2.301413\n",
      "Epoch 76, loss: 2.301474\n",
      "Epoch 77, loss: 2.301946\n",
      "Epoch 78, loss: 2.301585\n",
      "Epoch 79, loss: 2.300485\n",
      "Epoch 80, loss: 2.302481\n",
      "Epoch 81, loss: 2.301665\n",
      "Epoch 82, loss: 2.302147\n",
      "Epoch 83, loss: 2.301635\n",
      "Epoch 84, loss: 2.300948\n",
      "Epoch 85, loss: 2.301225\n",
      "Epoch 86, loss: 2.301106\n",
      "Epoch 87, loss: 2.302177\n",
      "Epoch 88, loss: 2.301870\n",
      "Epoch 89, loss: 2.302301\n",
      "Epoch 90, loss: 2.301189\n",
      "Epoch 91, loss: 2.301309\n",
      "Epoch 92, loss: 2.301718\n",
      "Epoch 93, loss: 2.302704\n",
      "Epoch 94, loss: 2.302304\n",
      "Epoch 95, loss: 2.301090\n",
      "Epoch 96, loss: 2.301631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 2.302004\n",
      "Epoch 98, loss: 2.301276\n",
      "Epoch 99, loss: 2.301358\n",
      "Epoch 100, loss: 2.301265\n",
      "Epoch 101, loss: 2.302163\n",
      "Epoch 102, loss: 2.301310\n",
      "Epoch 103, loss: 2.301126\n",
      "Epoch 104, loss: 2.301277\n",
      "Epoch 105, loss: 2.302362\n",
      "Epoch 106, loss: 2.301593\n",
      "Epoch 107, loss: 2.301462\n",
      "Epoch 108, loss: 2.302452\n",
      "Epoch 109, loss: 2.302349\n",
      "Epoch 110, loss: 2.300532\n",
      "Epoch 111, loss: 2.300761\n",
      "Epoch 112, loss: 2.301019\n",
      "Epoch 113, loss: 2.300554\n",
      "Epoch 114, loss: 2.300401\n",
      "Epoch 115, loss: 2.301676\n",
      "Epoch 116, loss: 2.302088\n",
      "Epoch 117, loss: 2.301514\n",
      "Epoch 118, loss: 2.301605\n",
      "Epoch 119, loss: 2.301993\n",
      "Epoch 120, loss: 2.302446\n",
      "Epoch 121, loss: 2.303151\n",
      "Epoch 122, loss: 2.301022\n",
      "Epoch 123, loss: 2.301568\n",
      "Epoch 124, loss: 2.301296\n",
      "Epoch 125, loss: 2.302223\n",
      "Epoch 126, loss: 2.302829\n",
      "Epoch 127, loss: 2.302186\n",
      "Epoch 128, loss: 2.300724\n",
      "Epoch 129, loss: 2.300979\n",
      "Epoch 130, loss: 2.302279\n",
      "Epoch 131, loss: 2.300683\n",
      "Epoch 132, loss: 2.301268\n",
      "Epoch 133, loss: 2.301494\n",
      "Epoch 134, loss: 2.300601\n",
      "Epoch 135, loss: 2.301180\n",
      "Epoch 136, loss: 2.301353\n",
      "Epoch 137, loss: 2.300731\n",
      "Epoch 138, loss: 2.300765\n",
      "Epoch 139, loss: 2.301364\n",
      "Epoch 140, loss: 2.301738\n",
      "Epoch 141, loss: 2.299722\n",
      "Epoch 142, loss: 2.302753\n",
      "Epoch 143, loss: 2.301987\n",
      "Epoch 144, loss: 2.301019\n",
      "Epoch 145, loss: 2.301155\n",
      "Epoch 146, loss: 2.300136\n",
      "Epoch 147, loss: 2.300810\n",
      "Epoch 148, loss: 2.302389\n",
      "Epoch 149, loss: 2.301845\n",
      "Epoch 150, loss: 2.300617\n",
      "Epoch 151, loss: 2.300346\n",
      "Epoch 152, loss: 2.300793\n",
      "Epoch 153, loss: 2.300622\n",
      "Epoch 154, loss: 2.301759\n",
      "Epoch 155, loss: 2.300422\n",
      "Epoch 156, loss: 2.301191\n",
      "Epoch 157, loss: 2.300986\n",
      "Epoch 158, loss: 2.301513\n",
      "Epoch 159, loss: 2.300270\n",
      "Epoch 160, loss: 2.300344\n",
      "Epoch 161, loss: 2.300072\n",
      "Epoch 162, loss: 2.300239\n",
      "Epoch 163, loss: 2.299933\n",
      "Epoch 164, loss: 2.301690\n",
      "Epoch 165, loss: 2.300311\n",
      "Epoch 166, loss: 2.301122\n",
      "Epoch 167, loss: 2.300821\n",
      "Epoch 168, loss: 2.301353\n",
      "Epoch 169, loss: 2.301315\n",
      "Epoch 170, loss: 2.300395\n",
      "Epoch 171, loss: 2.300856\n",
      "Epoch 172, loss: 2.299837\n",
      "Epoch 173, loss: 2.301074\n",
      "Epoch 174, loss: 2.300972\n",
      "Epoch 175, loss: 2.299189\n",
      "Epoch 176, loss: 2.301072\n",
      "Epoch 177, loss: 2.300158\n",
      "Epoch 178, loss: 2.299067\n",
      "Epoch 179, loss: 2.300887\n",
      "Epoch 180, loss: 2.301236\n",
      "Epoch 181, loss: 2.301755\n",
      "Epoch 182, loss: 2.300947\n",
      "Epoch 183, loss: 2.301529\n",
      "Epoch 184, loss: 2.301313\n",
      "Epoch 185, loss: 2.299597\n",
      "Epoch 186, loss: 2.300958\n",
      "Epoch 187, loss: 2.301047\n",
      "Epoch 188, loss: 2.300277\n",
      "Epoch 189, loss: 2.299644\n",
      "Epoch 190, loss: 2.300806\n",
      "Epoch 191, loss: 2.300462\n",
      "Epoch 192, loss: 2.298689\n",
      "Epoch 193, loss: 2.302019\n",
      "Epoch 194, loss: 2.301071\n",
      "Epoch 195, loss: 2.300725\n",
      "Epoch 196, loss: 2.301066\n",
      "Epoch 197, loss: 2.301008\n",
      "Epoch 198, loss: 2.300919\n",
      "Epoch 199, loss: 2.300451\n",
      "Epoch 0, loss: 2.302773\n",
      "Epoch 1, loss: 2.303589\n",
      "Epoch 2, loss: 2.302858\n",
      "Epoch 3, loss: 2.301939\n",
      "Epoch 4, loss: 2.301979\n",
      "Epoch 5, loss: 2.302073\n",
      "Epoch 6, loss: 2.302608\n",
      "Epoch 7, loss: 2.302581\n",
      "Epoch 8, loss: 2.302897\n",
      "Epoch 9, loss: 2.302204\n",
      "Epoch 10, loss: 2.302497\n",
      "Epoch 11, loss: 2.302610\n",
      "Epoch 12, loss: 2.302573\n",
      "Epoch 13, loss: 2.302658\n",
      "Epoch 14, loss: 2.302786\n",
      "Epoch 15, loss: 2.302962\n",
      "Epoch 16, loss: 2.302433\n",
      "Epoch 17, loss: 2.302875\n",
      "Epoch 18, loss: 2.303079\n",
      "Epoch 19, loss: 2.302247\n",
      "Epoch 20, loss: 2.302802\n",
      "Epoch 21, loss: 2.302950\n",
      "Epoch 22, loss: 2.302000\n",
      "Epoch 23, loss: 2.302969\n",
      "Epoch 24, loss: 2.301207\n",
      "Epoch 25, loss: 2.302632\n",
      "Epoch 26, loss: 2.303840\n",
      "Epoch 27, loss: 2.302147\n",
      "Epoch 28, loss: 2.302499\n",
      "Epoch 29, loss: 2.302526\n",
      "Epoch 30, loss: 2.302398\n",
      "Epoch 31, loss: 2.301774\n",
      "Epoch 32, loss: 2.302388\n",
      "Epoch 33, loss: 2.302934\n",
      "Epoch 34, loss: 2.301595\n",
      "Epoch 35, loss: 2.301918\n",
      "Epoch 36, loss: 2.302423\n",
      "Epoch 37, loss: 2.302182\n",
      "Epoch 38, loss: 2.303679\n",
      "Epoch 39, loss: 2.302293\n",
      "Epoch 40, loss: 2.301506\n",
      "Epoch 41, loss: 2.302201\n",
      "Epoch 42, loss: 2.302302\n",
      "Epoch 43, loss: 2.302936\n",
      "Epoch 44, loss: 2.301978\n",
      "Epoch 45, loss: 2.302600\n",
      "Epoch 46, loss: 2.302493\n",
      "Epoch 47, loss: 2.301949\n",
      "Epoch 48, loss: 2.301099\n",
      "Epoch 49, loss: 2.301902\n",
      "Epoch 50, loss: 2.301295\n",
      "Epoch 51, loss: 2.302327\n",
      "Epoch 52, loss: 2.302361\n",
      "Epoch 53, loss: 2.301944\n",
      "Epoch 54, loss: 2.301648\n",
      "Epoch 55, loss: 2.300560\n",
      "Epoch 56, loss: 2.301815\n",
      "Epoch 57, loss: 2.301108\n",
      "Epoch 58, loss: 2.301674\n",
      "Epoch 59, loss: 2.301081\n",
      "Epoch 60, loss: 2.301973\n",
      "Epoch 61, loss: 2.301390\n",
      "Epoch 62, loss: 2.301985\n",
      "Epoch 63, loss: 2.301817\n",
      "Epoch 64, loss: 2.301854\n",
      "Epoch 65, loss: 2.301503\n",
      "Epoch 66, loss: 2.302716\n",
      "Epoch 67, loss: 2.302286\n",
      "Epoch 68, loss: 2.302478\n",
      "Epoch 69, loss: 2.301609\n",
      "Epoch 70, loss: 2.302362\n",
      "Epoch 71, loss: 2.300996\n",
      "Epoch 72, loss: 2.301761\n",
      "Epoch 73, loss: 2.301077\n",
      "Epoch 74, loss: 2.302502\n",
      "Epoch 75, loss: 2.301074\n",
      "Epoch 76, loss: 2.303046\n",
      "Epoch 77, loss: 2.301484\n",
      "Epoch 78, loss: 2.301561\n",
      "Epoch 79, loss: 2.302319\n",
      "Epoch 80, loss: 2.301274\n",
      "Epoch 81, loss: 2.303301\n",
      "Epoch 82, loss: 2.302512\n",
      "Epoch 83, loss: 2.301080\n",
      "Epoch 84, loss: 2.301477\n",
      "Epoch 85, loss: 2.302462\n",
      "Epoch 86, loss: 2.302495\n",
      "Epoch 87, loss: 2.301829\n",
      "Epoch 88, loss: 2.301764\n",
      "Epoch 89, loss: 2.301190\n",
      "Epoch 90, loss: 2.301562\n",
      "Epoch 91, loss: 2.301842\n",
      "Epoch 92, loss: 2.299485\n",
      "Epoch 93, loss: 2.302126\n",
      "Epoch 94, loss: 2.301478\n",
      "Epoch 95, loss: 2.301588\n",
      "Epoch 96, loss: 2.302086\n",
      "Epoch 97, loss: 2.301243\n",
      "Epoch 98, loss: 2.301161\n",
      "Epoch 99, loss: 2.301146\n",
      "Epoch 100, loss: 2.300876\n",
      "Epoch 101, loss: 2.302186\n",
      "Epoch 102, loss: 2.302036\n",
      "Epoch 103, loss: 2.301431\n",
      "Epoch 104, loss: 2.300859\n",
      "Epoch 105, loss: 2.301374\n",
      "Epoch 106, loss: 2.300591\n",
      "Epoch 107, loss: 2.301434\n",
      "Epoch 108, loss: 2.301639\n",
      "Epoch 109, loss: 2.302314\n",
      "Epoch 110, loss: 2.301965\n",
      "Epoch 111, loss: 2.300751\n",
      "Epoch 112, loss: 2.301107\n",
      "Epoch 113, loss: 2.300309\n",
      "Epoch 114, loss: 2.301958\n",
      "Epoch 115, loss: 2.300573\n",
      "Epoch 116, loss: 2.301583\n",
      "Epoch 117, loss: 2.301448\n",
      "Epoch 118, loss: 2.301840\n",
      "Epoch 119, loss: 2.300277\n",
      "Epoch 120, loss: 2.301396\n",
      "Epoch 121, loss: 2.301159\n",
      "Epoch 122, loss: 2.300103\n",
      "Epoch 123, loss: 2.300664\n",
      "Epoch 124, loss: 2.301787\n",
      "Epoch 125, loss: 2.300443\n",
      "Epoch 126, loss: 2.302484\n",
      "Epoch 127, loss: 2.302102\n",
      "Epoch 128, loss: 2.301871\n",
      "Epoch 129, loss: 2.301783\n",
      "Epoch 130, loss: 2.300427\n",
      "Epoch 131, loss: 2.300956\n",
      "Epoch 132, loss: 2.301135\n",
      "Epoch 133, loss: 2.300781\n",
      "Epoch 134, loss: 2.301626\n",
      "Epoch 135, loss: 2.300571\n",
      "Epoch 136, loss: 2.301812\n",
      "Epoch 137, loss: 2.302302\n",
      "Epoch 138, loss: 2.301425\n",
      "Epoch 139, loss: 2.301056\n",
      "Epoch 140, loss: 2.302710\n",
      "Epoch 141, loss: 2.300649\n",
      "Epoch 142, loss: 2.302937\n",
      "Epoch 143, loss: 2.300785\n",
      "Epoch 144, loss: 2.300361\n",
      "Epoch 145, loss: 2.301800\n",
      "Epoch 146, loss: 2.301052\n",
      "Epoch 147, loss: 2.300450\n",
      "Epoch 148, loss: 2.300544\n",
      "Epoch 149, loss: 2.300557\n",
      "Epoch 150, loss: 2.300807\n",
      "Epoch 151, loss: 2.301574\n",
      "Epoch 152, loss: 2.300712\n",
      "Epoch 153, loss: 2.301307\n",
      "Epoch 154, loss: 2.300349\n",
      "Epoch 155, loss: 2.299678\n",
      "Epoch 156, loss: 2.301463\n",
      "Epoch 157, loss: 2.300608\n",
      "Epoch 158, loss: 2.300963\n",
      "Epoch 159, loss: 2.299700\n",
      "Epoch 160, loss: 2.299943\n",
      "Epoch 161, loss: 2.300622\n",
      "Epoch 162, loss: 2.300970\n",
      "Epoch 163, loss: 2.300802\n",
      "Epoch 164, loss: 2.299677\n",
      "Epoch 165, loss: 2.300947\n",
      "Epoch 166, loss: 2.301885\n",
      "Epoch 167, loss: 2.300842\n",
      "Epoch 168, loss: 2.300447\n",
      "Epoch 169, loss: 2.301285\n",
      "Epoch 170, loss: 2.301017\n",
      "Epoch 171, loss: 2.299513\n",
      "Epoch 172, loss: 2.300843\n",
      "Epoch 173, loss: 2.301203\n",
      "Epoch 174, loss: 2.301571\n",
      "Epoch 175, loss: 2.301499\n",
      "Epoch 176, loss: 2.300016\n",
      "Epoch 177, loss: 2.300053\n",
      "Epoch 178, loss: 2.300988\n",
      "Epoch 179, loss: 2.301232\n",
      "Epoch 180, loss: 2.300993\n",
      "Epoch 181, loss: 2.301097\n",
      "Epoch 182, loss: 2.300252\n",
      "Epoch 183, loss: 2.299908\n",
      "Epoch 184, loss: 2.301781\n",
      "Epoch 185, loss: 2.300742\n",
      "Epoch 186, loss: 2.300216\n",
      "Epoch 187, loss: 2.299424\n",
      "Epoch 188, loss: 2.300391\n",
      "Epoch 189, loss: 2.301343\n",
      "Epoch 190, loss: 2.299797\n",
      "Epoch 191, loss: 2.299879\n",
      "Epoch 192, loss: 2.300926\n",
      "Epoch 193, loss: 2.300967\n",
      "Epoch 194, loss: 2.300133\n",
      "Epoch 195, loss: 2.300316\n",
      "Epoch 196, loss: 2.301341\n",
      "Epoch 197, loss: 2.302406\n",
      "Epoch 198, loss: 2.300113\n",
      "Epoch 199, loss: 2.300887\n",
      "Epoch 0, loss: 2.302133\n",
      "Epoch 1, loss: 2.302946\n",
      "Epoch 2, loss: 2.302879\n",
      "Epoch 3, loss: 2.303658\n",
      "Epoch 4, loss: 2.303317\n",
      "Epoch 5, loss: 2.301630\n",
      "Epoch 6, loss: 2.301660\n",
      "Epoch 7, loss: 2.302037\n",
      "Epoch 8, loss: 2.302724\n",
      "Epoch 9, loss: 2.301360\n",
      "Epoch 10, loss: 2.302859\n",
      "Epoch 11, loss: 2.302945\n",
      "Epoch 12, loss: 2.302742\n",
      "Epoch 13, loss: 2.301668\n",
      "Epoch 14, loss: 2.301769\n",
      "Epoch 15, loss: 2.302659\n",
      "Epoch 16, loss: 2.302973\n",
      "Epoch 17, loss: 2.301399\n",
      "Epoch 18, loss: 2.302478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 2.302659\n",
      "Epoch 20, loss: 2.302796\n",
      "Epoch 21, loss: 2.302164\n",
      "Epoch 22, loss: 2.302594\n",
      "Epoch 23, loss: 2.303413\n",
      "Epoch 24, loss: 2.302766\n",
      "Epoch 25, loss: 2.302437\n",
      "Epoch 26, loss: 2.302886\n",
      "Epoch 27, loss: 2.302055\n",
      "Epoch 28, loss: 2.302725\n",
      "Epoch 29, loss: 2.302417\n",
      "Epoch 30, loss: 2.301905\n",
      "Epoch 31, loss: 2.302062\n",
      "Epoch 32, loss: 2.302047\n",
      "Epoch 33, loss: 2.301432\n",
      "Epoch 34, loss: 2.300573\n",
      "Epoch 35, loss: 2.301218\n",
      "Epoch 36, loss: 2.302336\n",
      "Epoch 37, loss: 2.302672\n",
      "Epoch 38, loss: 2.301824\n",
      "Epoch 39, loss: 2.301961\n",
      "Epoch 40, loss: 2.301638\n",
      "Epoch 41, loss: 2.302695\n",
      "Epoch 42, loss: 2.302018\n",
      "Epoch 43, loss: 2.302044\n",
      "Epoch 44, loss: 2.301666\n",
      "Epoch 45, loss: 2.301678\n",
      "Epoch 46, loss: 2.301064\n",
      "Epoch 47, loss: 2.302460\n",
      "Epoch 48, loss: 2.301307\n",
      "Epoch 49, loss: 2.302604\n",
      "Epoch 50, loss: 2.302322\n",
      "Epoch 51, loss: 2.301969\n",
      "Epoch 52, loss: 2.302229\n",
      "Epoch 53, loss: 2.301945\n",
      "Epoch 54, loss: 2.301427\n",
      "Epoch 55, loss: 2.301778\n",
      "Epoch 56, loss: 2.302620\n",
      "Epoch 57, loss: 2.302596\n",
      "Epoch 58, loss: 2.301650\n",
      "Epoch 59, loss: 2.300870\n",
      "Epoch 60, loss: 2.302028\n",
      "Epoch 61, loss: 2.301341\n",
      "Epoch 62, loss: 2.302473\n",
      "Epoch 63, loss: 2.302525\n",
      "Epoch 64, loss: 2.301349\n",
      "Epoch 65, loss: 2.302038\n",
      "Epoch 66, loss: 2.301373\n",
      "Epoch 67, loss: 2.302054\n",
      "Epoch 68, loss: 2.302716\n",
      "Epoch 69, loss: 2.302699\n",
      "Epoch 70, loss: 2.301188\n",
      "Epoch 71, loss: 2.301854\n",
      "Epoch 72, loss: 2.300227\n",
      "Epoch 73, loss: 2.302826\n",
      "Epoch 74, loss: 2.302543\n",
      "Epoch 75, loss: 2.302170\n",
      "Epoch 76, loss: 2.301719\n",
      "Epoch 77, loss: 2.302178\n",
      "Epoch 78, loss: 2.301143\n",
      "Epoch 79, loss: 2.301661\n",
      "Epoch 80, loss: 2.301392\n",
      "Epoch 81, loss: 2.302269\n",
      "Epoch 82, loss: 2.301208\n",
      "Epoch 83, loss: 2.302121\n",
      "Epoch 84, loss: 2.301947\n",
      "Epoch 85, loss: 2.301060\n",
      "Epoch 86, loss: 2.301999\n",
      "Epoch 87, loss: 2.301576\n",
      "Epoch 88, loss: 2.302010\n",
      "Epoch 89, loss: 2.302051\n",
      "Epoch 90, loss: 2.303637\n",
      "Epoch 91, loss: 2.301180\n",
      "Epoch 92, loss: 2.302559\n",
      "Epoch 93, loss: 2.301275\n",
      "Epoch 94, loss: 2.302481\n",
      "Epoch 95, loss: 2.300723\n",
      "Epoch 96, loss: 2.302065\n",
      "Epoch 97, loss: 2.301039\n",
      "Epoch 98, loss: 2.302193\n",
      "Epoch 99, loss: 2.300702\n",
      "Epoch 100, loss: 2.300072\n",
      "Epoch 101, loss: 2.301111\n",
      "Epoch 102, loss: 2.300737\n",
      "Epoch 103, loss: 2.302365\n",
      "Epoch 104, loss: 2.302458\n",
      "Epoch 105, loss: 2.301854\n",
      "Epoch 106, loss: 2.300969\n",
      "Epoch 107, loss: 2.301381\n",
      "Epoch 108, loss: 2.301477\n",
      "Epoch 109, loss: 2.301073\n",
      "Epoch 110, loss: 2.300156\n",
      "Epoch 111, loss: 2.302292\n",
      "Epoch 112, loss: 2.301564\n",
      "Epoch 113, loss: 2.300803\n",
      "Epoch 114, loss: 2.301155\n",
      "Epoch 115, loss: 2.300939\n",
      "Epoch 116, loss: 2.302361\n",
      "Epoch 117, loss: 2.301491\n",
      "Epoch 118, loss: 2.301171\n",
      "Epoch 119, loss: 2.301956\n",
      "Epoch 120, loss: 2.301783\n",
      "Epoch 121, loss: 2.302048\n",
      "Epoch 122, loss: 2.301398\n",
      "Epoch 123, loss: 2.301289\n",
      "Epoch 124, loss: 2.300231\n",
      "Epoch 125, loss: 2.301546\n",
      "Epoch 126, loss: 2.300554\n",
      "Epoch 127, loss: 2.300114\n",
      "Epoch 128, loss: 2.302071\n",
      "Epoch 129, loss: 2.300844\n",
      "Epoch 130, loss: 2.300342\n",
      "Epoch 131, loss: 2.300692\n",
      "Epoch 132, loss: 2.300953\n",
      "Epoch 133, loss: 2.301220\n",
      "Epoch 134, loss: 2.301030\n",
      "Epoch 135, loss: 2.301118\n",
      "Epoch 136, loss: 2.300377\n",
      "Epoch 137, loss: 2.302856\n",
      "Epoch 138, loss: 2.301435\n",
      "Epoch 139, loss: 2.300849\n",
      "Epoch 140, loss: 2.302030\n",
      "Epoch 141, loss: 2.301404\n",
      "Epoch 142, loss: 2.301772\n",
      "Epoch 143, loss: 2.300886\n",
      "Epoch 144, loss: 2.299714\n",
      "Epoch 145, loss: 2.300652\n",
      "Epoch 146, loss: 2.301717\n",
      "Epoch 147, loss: 2.302366\n",
      "Epoch 148, loss: 2.300469\n",
      "Epoch 149, loss: 2.301069\n",
      "Epoch 150, loss: 2.300997\n",
      "Epoch 151, loss: 2.299688\n",
      "Epoch 152, loss: 2.301764\n",
      "Epoch 153, loss: 2.300720\n",
      "Epoch 154, loss: 2.301394\n",
      "Epoch 155, loss: 2.300619\n",
      "Epoch 156, loss: 2.301488\n",
      "Epoch 157, loss: 2.300965\n",
      "Epoch 158, loss: 2.301849\n",
      "Epoch 159, loss: 2.300929\n",
      "Epoch 160, loss: 2.300965\n",
      "Epoch 161, loss: 2.302019\n",
      "Epoch 162, loss: 2.300756\n",
      "Epoch 163, loss: 2.299450\n",
      "Epoch 164, loss: 2.300041\n",
      "Epoch 165, loss: 2.300019\n",
      "Epoch 166, loss: 2.301070\n",
      "Epoch 167, loss: 2.300386\n",
      "Epoch 168, loss: 2.299439\n",
      "Epoch 169, loss: 2.300282\n",
      "Epoch 170, loss: 2.300897\n",
      "Epoch 171, loss: 2.301880\n",
      "Epoch 172, loss: 2.300200\n",
      "Epoch 173, loss: 2.300663\n",
      "Epoch 174, loss: 2.300318\n",
      "Epoch 175, loss: 2.302247\n",
      "Epoch 176, loss: 2.300262\n",
      "Epoch 177, loss: 2.300075\n",
      "Epoch 178, loss: 2.299128\n",
      "Epoch 179, loss: 2.301732\n",
      "Epoch 180, loss: 2.298706\n",
      "Epoch 181, loss: 2.300820\n",
      "Epoch 182, loss: 2.301458\n",
      "Epoch 183, loss: 2.300143\n",
      "Epoch 184, loss: 2.301892\n",
      "Epoch 185, loss: 2.300427\n",
      "Epoch 186, loss: 2.300392\n",
      "Epoch 187, loss: 2.300208\n",
      "Epoch 188, loss: 2.301508\n",
      "Epoch 189, loss: 2.301971\n",
      "Epoch 190, loss: 2.300396\n",
      "Epoch 191, loss: 2.301389\n",
      "Epoch 192, loss: 2.300605\n",
      "Epoch 193, loss: 2.300721\n",
      "Epoch 194, loss: 2.299770\n",
      "Epoch 195, loss: 2.299451\n",
      "Epoch 196, loss: 2.300245\n",
      "Epoch 197, loss: 2.301016\n",
      "Epoch 198, loss: 2.301587\n",
      "Epoch 199, loss: 2.299325\n",
      "best validation accuracy achieved: 0.230000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifier.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T19:44:00.857473Z",
     "start_time": "2020-02-12T19:44:00.838312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.195000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
